{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 20:28:41.862543: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-27 20:28:41.885560: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 20:28:41.885588: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 20:28:41.885604: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 20:28:41.889965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to calculate FPS\n",
    "COUNTER, FPS = 0, 0\n",
    "START_TIME = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model: str, num_hands: int,\n",
    "        min_hand_detection_confidence: float,\n",
    "        min_hand_presence_confidence: float, min_tracking_confidence: float,\n",
    "        camera_id: int, width: int, height: int) -> None:\n",
    "  \"\"\"Continuously run inference on images acquired from the camera.\n",
    "\n",
    "  Args:\n",
    "      model: Name of the gesture recognition model bundle.\n",
    "      num_hands: Max number of hands can be detected by the recognizer.\n",
    "      min_hand_detection_confidence: The minimum confidence score for hand\n",
    "        detection to be considered successful.\n",
    "      min_hand_presence_confidence: The minimum confidence score of hand\n",
    "        presence score in the hand landmark detection.\n",
    "      min_tracking_confidence: The minimum confidence score for the hand\n",
    "        tracking to be considered successful.\n",
    "      camera_id: The camera id to be passed to OpenCV.\n",
    "      width: The width of the frame captured from the camera.\n",
    "      height: The height of the frame captured from the camera.\n",
    "  \"\"\"\n",
    "\n",
    "  # Start capturing video input from the camera\n",
    "  cap = cv2.VideoCapture(camera_id)\n",
    "  cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "  cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "  # Visualization parameters\n",
    "  row_size = 50  # pixels\n",
    "  left_margin = 24  # pixels\n",
    "  text_color = (0, 0, 0)  # black\n",
    "  font_size = 1\n",
    "  font_thickness = 1\n",
    "  fps_avg_frame_count = 10\n",
    "\n",
    "  # Label box parameters\n",
    "  label_text_color = (255, 255, 255)  # white\n",
    "  label_font_size = 1\n",
    "  label_thickness = 2\n",
    "\n",
    "  recognition_frame = None\n",
    "  recognition_result_list = []\n",
    "\n",
    "  def save_result(result: vision.GestureRecognizerResult,\n",
    "                  unused_output_image: mp.Image, timestamp_ms: int):\n",
    "      global FPS, COUNTER, START_TIME\n",
    "\n",
    "      # Calculate the FPS\n",
    "      if COUNTER % fps_avg_frame_count == 0:\n",
    "          FPS = fps_avg_frame_count / (time.time() - START_TIME)\n",
    "          START_TIME = time.time()\n",
    "\n",
    "      recognition_result_list.append(result)\n",
    "      COUNTER += 1\n",
    "\n",
    "  # Initialize the gesture recognizer model\n",
    "  base_options = python.BaseOptions(model_asset_path=model)\n",
    "  options = vision.GestureRecognizerOptions(base_options=base_options,\n",
    "                                          running_mode=vision.RunningMode.LIVE_STREAM,\n",
    "                                          num_hands=num_hands,\n",
    "                                          min_hand_detection_confidence=min_hand_detection_confidence,\n",
    "                                          min_hand_presence_confidence=min_hand_presence_confidence,\n",
    "                                          min_tracking_confidence=min_tracking_confidence,\n",
    "                                          result_callback=save_result)\n",
    "  recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "  # Continuously capture images from the camera and run inference\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      sys.exit(\n",
    "          'ERROR: Unable to read from webcam. Please verify your webcam settings.'\n",
    "      )\n",
    "\n",
    "    image = cv2.flip(image, 1)\n",
    "\n",
    "    # Convert the image from BGR to RGB as required by the TFLite model.\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "\n",
    "    # Run gesture recognizer using the model.\n",
    "    recognizer.recognize_async(mp_image, time.time_ns() // 1_000_000)\n",
    "\n",
    "    # Show the FPS\n",
    "    fps_text = 'FPS = {:.1f}'.format(FPS)\n",
    "    text_location = (left_margin, row_size)\n",
    "    current_frame = image\n",
    "    cv2.putText(current_frame, fps_text, text_location, cv2.FONT_HERSHEY_DUPLEX,\n",
    "                font_size, text_color, font_thickness, cv2.LINE_AA)\n",
    "\n",
    "    if recognition_result_list:\n",
    "      # Draw landmarks and write the text for each hand.\n",
    "      for hand_index, hand_landmarks in enumerate(\n",
    "          recognition_result_list[0].hand_landmarks):\n",
    "        # Calculate the bounding box of the hand\n",
    "        x_min = min([landmark.x for landmark in hand_landmarks])\n",
    "        y_min = min([landmark.y for landmark in hand_landmarks])\n",
    "        y_max = max([landmark.y for landmark in hand_landmarks])\n",
    "\n",
    "        # Convert normalized coordinates to pixel values\n",
    "        frame_height, frame_width = current_frame.shape[:2]\n",
    "        x_min_px = int(x_min * frame_width)\n",
    "        y_min_px = int(y_min * frame_height)\n",
    "        y_max_px = int(y_max * frame_height)\n",
    "\n",
    "        # Get gesture classification results\n",
    "        if recognition_result_list[0].gestures:\n",
    "          gesture = recognition_result_list[0].gestures[hand_index]\n",
    "          category_name = gesture[0].category_name\n",
    "          score = round(gesture[0].score, 2)\n",
    "          result_text = f'{category_name} ({score})'\n",
    "\n",
    "          # Compute text size\n",
    "          text_size = \\\n",
    "          cv2.getTextSize(result_text, cv2.FONT_HERSHEY_DUPLEX, label_font_size,\n",
    "                          label_thickness)[0]\n",
    "          text_width, text_height = text_size\n",
    "\n",
    "          # Calculate text position (above the hand)\n",
    "          text_x = x_min_px\n",
    "          text_y = y_min_px - 10  # Adjust this value as needed\n",
    "\n",
    "          # Make sure the text is within the frame boundaries\n",
    "          if text_y < 0:\n",
    "            text_y = y_max_px + text_height\n",
    "\n",
    "          # Draw the text\n",
    "          cv2.putText(current_frame, result_text, (text_x, text_y),\n",
    "                      cv2.FONT_HERSHEY_DUPLEX, label_font_size,\n",
    "                      label_text_color, label_thickness, cv2.LINE_AA)\n",
    "\n",
    "        # Draw hand landmarks on the frame\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "          landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y,\n",
    "                                          z=landmark.z) for landmark in\n",
    "          hand_landmarks\n",
    "        ])\n",
    "        mp_drawing.draw_landmarks(\n",
    "          current_frame,\n",
    "          hand_landmarks_proto,\n",
    "          mp_hands.HAND_CONNECTIONS,\n",
    "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "          mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "      recognition_frame = current_frame\n",
    "      recognition_result_list.clear()\n",
    "\n",
    "    if recognition_frame is not None:\n",
    "        cv2.imshow('gesture_recognition', recognition_frame)\n",
    "\n",
    "    # Stop the program if the ESC key is pressed.\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "  recognizer.close()\n",
    "  cap.release()\n",
    "  cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] [--numHands NUMHANDS]\n",
      "                             [--minHandDetectionConfidence MINHANDDETECTIONCONFIDENCE]\n",
      "                             [--minHandPresenceConfidence MINHANDPRESENCECONFIDENCE]\n",
      "                             [--minTrackingConfidence MINTRACKINGCONFIDENCE]\n",
      "                             [--cameraId CAMERAID] [--frameWidth FRAMEWIDTH]\n",
      "                             [--frameHeight FRAMEHEIGHT]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/home/jaerock/.local/share/jupyter/runtime/kernel-v2-8361mmwHQnORbKqI.json could match --frameWidth, --frameHeight\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaerock/anaconda3/envs/ece5831-2023-py3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "  parser = argparse.ArgumentParser(\n",
    "      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Name of gesture recognition model.',\n",
    "      required=False,\n",
    "      default='gesture_recognizer.task')\n",
    "  parser.add_argument(\n",
    "      '--numHands',\n",
    "      help='Max number of hands that can be detected by the recognizer.',\n",
    "      required=False,\n",
    "      default=1)\n",
    "  parser.add_argument(\n",
    "      '--minHandDetectionConfidence',\n",
    "      help='The minimum confidence score for hand detection to be considered '\n",
    "           'successful.',\n",
    "      required=False,\n",
    "      default=0.5)\n",
    "  parser.add_argument(\n",
    "      '--minHandPresenceConfidence',\n",
    "      help='The minimum confidence score of hand presence score in the hand '\n",
    "           'landmark detection.',\n",
    "      required=False,\n",
    "      default=0.5)\n",
    "  parser.add_argument(\n",
    "      '--minTrackingConfidence',\n",
    "      help='The minimum confidence score for the hand tracking to be '\n",
    "           'considered successful.',\n",
    "      required=False,\n",
    "      default=0.5)\n",
    "  # Finding the camera ID can be very reliant on platform-dependent methods.\n",
    "  # One common approach is to use the fact that camera IDs are usually indexed sequentially by the OS, starting from 0.\n",
    "  # Here, we use OpenCV and create a VideoCapture object for each potential ID with 'cap = cv2.VideoCapture(i)'.\n",
    "  # If 'cap' is None or not 'cap.isOpened()', it indicates the camera ID is not available.\n",
    "  parser.add_argument(\n",
    "      '--cameraId', help='Id of camera.', required=False, default=0)\n",
    "  parser.add_argument(\n",
    "      '--frameWidth',\n",
    "      help='Width of frame to capture from camera.',\n",
    "      required=False,\n",
    "      default=640)\n",
    "  parser.add_argument(\n",
    "      '--frameHeight',\n",
    "      help='Height of frame to capture from camera.',\n",
    "      required=False,\n",
    "      default=480)\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  run(args.model, int(args.numHands), args.minHandDetectionConfidence,\n",
    "      args.minHandPresenceConfidence, args.minTrackingConfidence,\n",
    "      int(args.cameraId), args.frameWidth, args.frameHeight)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece5831-2023-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
